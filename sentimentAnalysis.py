# -*- coding: utf-8 -*-
"""ASS2-1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AtIDFp9UbjFE9_2RXrhXbIScEolqSdSd

#Importing libraries
"""

import pandas as pd
import numpy as np
import nltk 
from nltk.corpus import stopwords 
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.model_selection import train_test_split
from sklearn import naive_bayes

"""#Importing dataset"""

df = pd.read_csv('Trainset.csv', encoding='gbk')

df.head()

"""#Checking the distribution of rating"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
print('% for default')
print(round(df.rating.value_counts(normalize=True)*100,2))
round(df.rating.value_counts(normalize=True)*100,2).plot(kind='bar')
plt.title('%distribution by review type')
plt.show()

"""#naive bayes"""

stopset = set (stopwords.words('english'))
vectorizer = TfidfVectorizer(use_idf=True, lowercase=True,strip_accents='ascii', stop_words=stopset)

y = df.rating

x = vectorizer.fit_transform (df.review)

x_train,x_test, y_train, y_test = train_test_split(x,y, test_size=0.01, random_state = 42)

clf = naive_bayes.ComplementNB()
clf.fit(x_train, y_train)

test = pd.read_csv('Testset.csv', encoding = 'unicode_escape')
w = test.iloc[:, -1].values
rest_review_vector = vectorizer.transform(w)
print(type(rest_review_vector))
z = rest_review_vector
print(clf.predict(z))

test.drop(columns = ['review'], inplace = True)
test['rating']=pd.Series(clf.predict(z))

test.head()

test.to_csv('attempt5.csv')

"""#SVM"""

test = pd.read_csv('Testset.csv', encoding = 'unicode_escape')

from sklearn.feature_extraction.text import TfidfVectorizer
# Create feature vectors
vectorizer = TfidfVectorizer(min_df = 5,
                             max_df = 0.8,
                             sublinear_tf = True,
                             use_idf = True)
train_vectors = vectorizer.fit_transform(df['review'])
test_vectors = vectorizer.transform(test['review'])

import time
from sklearn import svm
from sklearn.metrics import classification_report
# Perform classification with SVM, kernel=linear
classifier_linear = svm.SVC(kernel='linear')
t0 = time.time()
classifier_linear.fit(train_vectors, df['rating'])
t1 = time.time()
prediction_linear = classifier_linear.predict(test_vectors)
t2 = time.time()
time_linear_train = t1-t0
time_linear_predict = t2-t1
# results
print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))

for review in test.review:
  review_vector = vectorizer.transform([review]) # vectorizing
  answer = np.append(answer,classifier_linear.predict(review_vector))


len(answer)

test.drop(columns = ['review'], inplace = True)
test['rating']=pd.Series(answer)
test.head()

test.to_csv('attempt6.csv')

"""#random forest"""

import re
nltk.download('stopwords')

features = df.iloc[:, 2].values
labels = df.iloc[:, 1].values

processed_features = []

for sentence in range(0, len(features)):
    # Remove all the special characters
    processed_feature = re.sub(r'\W', ' ', str(features[sentence]))

    # remove all single characters
    processed_feature= re.sub(r'\s+[a-zA-Z]\s+', ' ', processed_feature)

    # Remove single characters from the start
    processed_feature = re.sub(r'\^[a-zA-Z]\s+', ' ', processed_feature) 

    # Substituting multiple spaces with single space
    processed_feature = re.sub(r'\s+', ' ', processed_feature, flags=re.I)

    # Removing prefixed 'b'
    processed_feature = re.sub(r'^b\s+', '', processed_feature)

    # Converting to Lowercase
    processed_feature = processed_feature.lower()

    processed_features.append(processed_feature)

from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer


vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))
processed_features = vectorizer.fit_transform(processed_features).toarray()

X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.000001, random_state=0)

from sklearn.ensemble import RandomForestClassifier

text_classifier = RandomForestClassifier(n_estimators=18000,max_depth=600, random_state=2)
text_classifier.fit(X_train, y_train)

test = pd.read_csv('Testset.csv', encoding = 'unicode_escape')
w = test.iloc[:, -1].values
rest_review_vector = vectorizer.transform(w)
z = rest_review_vector
final_result = text_classifier.predict(z)
print(final_result)

test.drop(columns = ['review'], inplace = True)
test['rating']=pd.Series(final_result)
test.head()

test.to_csv('attempt.csv')

"""#Decision tree


"""

from sklearn.tree import DecisionTreeClassifier

clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=10000000000, min_samples_split = 10)

clf_tree.fit(X_train, y_train)

test = pd.read_csv('Testset.csv', encoding = 'unicode_escape')
w = test.iloc[:, -1].values
rest_review_vector = vectorizer.transform(w)
z = rest_review_vector
final_result = clf_tree.predict(z)
print(final_result)

test.drop(columns = ['review'], inplace = True)
test['rating']=pd.Series(final_result)
test.head()

test.to_csv('attempt26.csv')

"""#gradient boost"""

i = 29

from sklearn.ensemble import GradientBoostingClassifier
gra_boost = GradientBoostingClassifier(max_depth = 8, learning_rate = 0.1,n_estimators = 1000 ,random_state=0)
gra_boost.fit(X_train, y_train)

test = pd.read_csv('Testset.csv', encoding = 'unicode_escape')
w = test.iloc[:, -1].values
z = vectorizer.transform(w)
final_result = gra_boost.predict(z)
print(final_result)

test.drop(columns = ['review'], inplace = True)
test['rating']=pd.Series(final_result)
test.head()
i=i+1
str1 = 'attempt'+str(i)+'.csv'
test.to_csv(str1)